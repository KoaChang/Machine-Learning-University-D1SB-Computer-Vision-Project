from typing import Optional, Tuple, Union

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
import torch


class Kernel(object):
    """Base class for kernel functions."""

    def __init__(self):
        pass

    def check_inputs(self, X0, X1):
        assert X0.dim() == X1.dim() == 2
        assert X0.size(1) == X1.size(1)
        assert X0.dtype is X1.dtype


class RBFKernel(Kernel):
    """A radial basis function kernel.

    This kernel has one hyperparameter, a scalar lengthscale. If this is set
    to `None` (default), the lengthscale will be set adaptively, at _each_ call
    to the kernel, via the median heuristic.

    Args:
        lengthscale: The kernel lengthscale. If `None` (default), this is set
            automatically via the median heuristic. Note: In this case, the
            lengthscale will be reset at each call to the kernel.
    """

    def __init__(self, lengthscale: Optional[float] = None):
        super().__init__()
        self._lengthscale = lengthscale

    @torch.no_grad()
    def __call__(self, X0: torch.Tensor, X1: torch.Tensor):
        self.check_inputs(X0, X1)
        dists = torch.cdist(X0, X1)
        lengthscale = self._lengthscale or torch.median(dists)
        return torch.exp(-0.5 * dists ** 2 / lengthscale ** 2)


class KroneckerDeltaKernel(Kernel):
    """A Kronecker-delta kernel."""

    def __init__(self):
        super().__init__()

    def __call__(self, X0, X1):
        self.check_inputs(X0, X1)
        # TODO(balleslb): Computing distances is overkill here.
        dists = torch.cdist(X0.float(), X1.float())
        return (dists == 0).float()


def mmd_gram(
            K: torch.Tensor,
            z: torch.Tensor,
            num_permutations: int = 0
        ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    """Maximum mean discrepancy based on a precomputed kernel-Gram matrix.

    This computes the test statistic and (optionally) p-value to conduct an
    MMD two-sample test to decide whether two sets are generated by the same
    distribution. The inputs are passed implicitly in the form of a kernel Gram
    matrix, evaluated across the union of both sets, and a binary vector
    indicating the assignments of data points to the two sets. I.e., a value
    of `z[i] = 0` indicates that the `i`-th data point belongs to set zero.
    Optionally, a permutation test is carried out and a p-value is returned
    alongside the raw test statistic.

    MMD tests have been proposed by

    [1] Gretton, A., et al. A kernel two-sample test. JMLR (2012).

    Args:
        K: A tensor containing a kernel-Gram matrix (size `(n, n)`).
        z: A binary vector of length `n` indicating the partition.
        num_permutations: If this is a positive number, a permutation test will
            be carried out and a p-value will be returned.

    Returns:
        A tuple `(t, p)` of two scalar floats, where `t` is the value of the
        MMD test statistic and `p` is the p-value (or `None` if
        `num_permutations=0`).
    """
    n = K.size(0)
    assert K.size(1) == n
    assert z.size() == (n,)
    inds_0 = torch.where(z == 0)[0]
    inds_1 = torch.where(z == 1)[0]
    n0 = len(inds_0)
    n1 = len(inds_1)

    # Compute value of MMD statistic, see Eq. (5) in [1].
    mmd = K[inds_0][:, inds_0].sum() / (n0 * (n0-1))  \
        + K[inds_1][:, inds_1].sum() / (n1 * (n1-1))  \
        - 2*K[inds_0][:, inds_1].mean()

    if num_permutations == 0:
        return mmd, None

    # Permutation test: Randomize the assignment vector z, compute MMD, and
    # count how often we exceed the value obtained with original z.
    cnt = 0
    for _ in range(num_permutations):
        z_ = torch.zeros(n, device=K.device)
        pi = torch.randperm(n, device=K.device)
        z_[pi[:n1]] = 1
        mmd0, _ = mmd_gram(K, z_, num_permutations=0)
        if mmd0 > mmd:
            cnt += 1
    p_val = torch.tensor(cnt / num_permutations)

    return mmd, p_val


def mmd(X0: torch.Tensor,
        X1: torch.Tensor,
        kernel: Kernel,
        num_permutations: int = 0
        ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    """Compute MMD between two samples.

    Optionally, return an estimated p-value based on a permutation test.

    Args:
        X0: First sample, shape (n0, dx).
        X1: Second sample, shape (n1, dx).
        kernel: Kernel function to use.
        num_permutations: If this is a positive number, a permutation test will
            be carried out and a p-value will be returned.

    Returns:
        A tuple `(t, p)` where `t` is the value of the MMD test statistic and
        `p` is the p-value (or `None` if `num_permutations=0`).
    """
    X = torch.cat([X0, X1], dim=0)
    z = torch.cat([torch.zeros(X0.size(0)), torch.ones(X1.size(0))], dim=0)
    K = kernel(X, X)
    assert not torch.any(torch.isnan(K))
    return mmd_gram(K, z, num_permutations)


def mmd_joint(X0: torch.Tensor,
              Y0: torch.Tensor,
              X1: torch.Tensor,
              Y1: torch.Tensor,
              kernel_x: Kernel,
              kernel_y: Kernel,
              num_permutations: int = 0) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    """Compute MMD between to samples of a joint distribution.

    This will run an MMD test on the joint distribution using a product kernel
    obtained by multiplying the two kernels `kernel_x` and `kernel_y`.
    Optionally, return an estimated p-value based on a permutation test.

    Args:
        X0: x-values for the first sample, shape (n0, dx).
        Y0: y-values for the first sample, shape (n0, dy).
        X1: x-values for the second sample, shape (n1, dx).
        Y1: y-values for the second sample, shape (n1, dy).
        kernel_x: Kernel function to use on x.
        kernel_y: Kernel function to use on y.
        num_permutations: If this is a positive number, a permutation test will
            be carried out and a p-value will be returned.

    Returns:
        A tuple `(t, p)` where `t` is the value of the MMD test statistic and
        `p` is the p-value (or `None` if `num_permutations=0`).
    """
    X = torch.cat([X0, X1], dim=0)
    Y = torch.cat([Y0, Y1], dim=0)
    K = kernel_x(X, X) * kernel_y(Y, Y)
    z = torch.cat([torch.zeros(X0.size(0)), torch.ones(X1.size(0))], dim=0)
    return mmd_gram(K, z, num_permutations)


def mmd_conditional_gram(
        K_x: torch.Tensor,
        K_y: torch.Tensor,
        z: torch.Tensor,
        propensity_scores: torch.Tensor,
        lam_0: float,
        lam_1: float,
        K_y_held: Optional[torch.Tensor] = None,
        num_permutations: int = 0,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
    """Conditional maximum mean discrepancy based on a precomputed kernel-Gram matrix.

    Args:
        K_x: A tensor (size `(n, n)`) of containg pairwise kernel evaluations
            of x values.
        K_y: A tensor (size `(n, n)`) of containg pairwise kernel evaluations
            of y values.
        z: A binary vector of length `n` indicating the partition.
        propensity_scores: TODO.
        lam_0:
        lam_1:
        K_y_held: An optional tensor of size `(n, n_h)` containing pairwise
            kernel evaluations between the y-values in our dataset and the
            y-values in a held-out of points. These are used to average the
            conditional MMD over y space.
        num_permutations: If this is a positive number, a permutation test will
            be carried out and a p-value will be returned.

    Returns:
        A tuple `(t, p)` of two scalar floats, where `t` is the value of the
        MMD test statistic and `p` is the p-value (or `None` if
        `num_permutations=0`).
    """
    n = K_x.size(0)
    assert K_x.size(1) == K_y.size(1) == K_y.size(0) == n
    assert z.size() == (n,)
    assert lam_0 > 0.0
    assert lam_1 > 0.0

    # If no held-out y-data, we re-use the y-data underlying K_y.
    if K_y_held is None:
        K_y_held = K_y.clone()

    assert K_y_held.size(0) == n

    inds_0 = torch.where(z == 0)[0]
    inds_1 = torch.where(z == 1)[0]
    n0 = len(inds_0)
    n1 = len(inds_1)

    K_x_00 = K_x[inds_0][:, inds_0]
    K_x_11 = K_x[inds_1][:, inds_1]
    K_x_01 = K_x[inds_0][:, inds_1]
    K_y_00 = K_y[inds_0][:, inds_0]
    K_y_11 = K_y[inds_1][:, inds_1]
    K_y_0h = K_y_held[inds_0, :]
    K_y_1h = K_y_held[inds_1, :]

    # Compute conditional MMD.
    A = torch.linalg.solve(K_y_00 + lam_0 * n0 * torch.eye(n0), K_y_0h)  # n0 x nh
    B = torch.linalg.solve(K_y_11 + lam_1 * n1 * torch.eye(n1), K_y_1h)  # n1 x nh

    mmd = ((A @ A.T) * K_x_00).sum() + ((B @ B.T) * K_x_11).sum() \
        - 2 * ((A @ B.T) * K_x_01).sum()

    if num_permutations == 0:
        return mmd, None

    cnt = 0
    for _ in range(num_permutations):
        z_ = torch.bernoulli(propensity_scores)
        mmd0, _ = mmd_conditional_gram(K_x, K_y, z_, propensity_scores, lam_0, lam_1, K_y_held, 0)
        if mmd0 > mmd:
            cnt += 1
    p_val = torch.tensor(cnt / num_permutations)

    return mmd, p_val


def sample_held_out_data(X0, Y0, X1, Y1, percentage):
    assert percentage
    n0 = X0.size(0)
    n1 = X1.size(0)
    n0_held = round(percentage * n0)
    n1_held = round(percentage * n1)
    rand_idx_0 = torch.randperm(n0)
    rand_idx_1 = torch.randperm(n1)
    X0, X0_held = torch.split(X0[rand_idx_0], [n0-n0_held, n0_held], dim=0)
    Y0, Y0_held, = torch.split(Y0[rand_idx_0], [n0-n0_held, n0_held], dim=0)
    X1, X1_held = torch.split(X1[rand_idx_1], [n1-n1_held, n1_held], dim=0)
    Y1, Y1_held, = torch.split(Y1[rand_idx_1], [n1-n1_held, n1_held], dim=0)
    X_held = torch.cat([X0_held, X1_held], dim=0)
    Y_held = torch.cat([Y0_held, Y1_held], dim=0)
    return X0, Y0, X1, Y1, X_held, Y_held


def mmd_conditional(
        X0: torch.Tensor,
        Y0: torch.Tensor,
        X1: torch.Tensor,
        Y1: torch.Tensor,
        kernel_x: Kernel,
        kernel_y: Kernel,
        lam_0: float = 0.0,
        lam_1: float = 0.0,
        prop_score_estimator: str = "SVM",
        prop_score_cutoff: float = 0.0,
        num_permutations: int = 0,
        percentage_held: float = 0.25):
    """Compute MMD between to samples of a conditional distribution.

    This will run an MMD test on the conditional distribution.
    Optionally, return an estimated p-value based on a permutation test.

    Args:
        X0: x-values for the first sample, shape (n0, dx).
        Y0: y-values for the first sample, shape (n0, dy).
        X1: x-values for the second sample, shape (n1, dx).
        Y1: y-values for the second sample, shape (n1, dy).
        kernel_x: Kernel function to use on x.
        kernel_y: Kernel function to use on y.
        lam_0: TODO
        lam_1: TODO
        num_permutations: If this is a positive number, a permutation test will
            be carried out and a p-value will be returned.
        percentage_held: TODO

    Returns:
        A tuple `(t, p)` where `t` is the value of the MMD test statistic and
        `p` is the p-value (or `None` if `num_permutations=0`).
    """
    X0, Y0, X1, Y1, _, Y_h = sample_held_out_data(X0, Y0, X1, Y1, percentage=percentage_held)
    X = torch.cat([X0, X1], dim=0)
    K_x = kernel_x(X, X)
    Y = torch.cat([Y0, Y1], dim=0)
    K_y = kernel_y(Y, Y)
    K_y_held = kernel_y(Y, Y_h)
    z = torch.cat([torch.zeros(X0.size(0)), torch.ones(X1.size(0))]).long()

    # For conditional permutation test, fit a domain classifier, i.e., a
    # classifier that learns to predict z from y. The probablity p(z=1|y) is
    # called a propensity score and can be used resample the assignments in the
    # z vector in a way that as consistent with the
    if prop_score_estimator == "SVM":
        clf = SVC(kernel="precomputed", C=1e12, class_weight="balanced", probability=True)
        clf.fit(K_y.clone().numpy(), z.clone().numpy())
        propensity_scores = torch.as_tensor(clf.predict_proba(K_y.clone().numpy()))[:, 1]
    elif prop_score_estimator == "GP":
        raise NotImplementedError()
    elif prop_score_estimator == "RF":
        clf = RandomForestClassifier()
        clf.fit(Y.clone().numpy(), z.clone().numpy())
        propensity_scores = torch.as_tensor(clf.predict_proba(Y.clone().numpy()))[:, 1]
    else:
        raise ValueError()

    if prop_score_cutoff:
        propensity_scores[propensity_scores < prop_score_cutoff] = 0.
        propensity_scores[propensity_scores > (1.  - prop_score_cutoff)] = 1.

    return mmd_conditional_gram(K_x, K_y, z, propensity_scores, lam_0, lam_1, K_y_held, num_permutations)
